{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815291c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff04a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_from_list(tensors):\n",
    "    pos_list = [t.shape[0] for t in tensors ]\n",
    "    batch = torch.cat(tensors, dim=0)\n",
    "    return batch, pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34aca770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_poses(poses, features_dim):\n",
    "    '''T[batch_size, K, 3] -> T[batch_size, 2K, D, D]'''\n",
    "    batch_size, K, _ = poses.shape\n",
    "    features_poses = []\n",
    "    visibilities = []\n",
    "    for keypoint in poses:\n",
    "        x, y = keypoint[:,0], keypoint[:,1]\n",
    "        visibilities.append(keypoint[:,2].unsqueeze(0))\n",
    "        features = []\n",
    "        for x_val, y_val in zip(x, y):\n",
    "            features.append(torch.full((1,1,features_dim,features_dim), x_val))\n",
    "            features.append(torch.full((1,1,features_dim,features_dim), y_val))\n",
    "        features = torch.cat(features, dim=1)\n",
    "        features_poses.append(features)\n",
    "    features_poses = torch.cat(features_poses, dim=0)\n",
    "    visibilities = torch.cat(visibilities, dim=0).unsqueeze(-1)\n",
    "    return features_poses, visibilities\n",
    "    \n",
    "    \n",
    "#     for k in range(poses.shape[1]):\n",
    "#         x = torch.cat([\n",
    "#             torch.full((batch_size, 1, features_dim, features_dim), v) \n",
    "#             for v in poses[:,k,0]\n",
    "#         ], dim=1)\n",
    "#         y = torch.cat([\n",
    "#             torch.full((batch_size, 1, features_dim, features_dim), v) \n",
    "#             for v in poses[:,k,1]\n",
    "#         ], dim=1)\n",
    "#         print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8cee53c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,5)/(2*torch.ones(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17b6d99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 256, 7, 7])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [torch.randn(10, 256, 7, 7) for _ in range(15)]\n",
    "batch_features, pos_list = batch_from_list(features)\n",
    "batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bec72782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, torch.Size([10, 256, 7, 7]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_features = list(batch_features.split(pos_list, 0))\n",
    "len(list_features), list_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "264adc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_poses = torch.randn(150, 14, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ab4b1958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 14, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f, v = preprocess_poses(batch_poses,7)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39af78f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, K, _ = batch_poses.shape\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67bc5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape\n",
    "p = F.avg_pool2d(f, 7)\n",
    "px = torch.cat([p[:,i] for i in range(0, p.shape[1], 2)], dim=1)\n",
    "py = torch.cat([p[:,i] for i in range(1, p.shape[1]+1, 2)], dim=1)\n",
    "p = torch.cat([px,py], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d462f3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150, 14, 1]), torch.Size([150, 14, 1]), torch.Size([150, 14, 2]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.shape, py.shape, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c5a983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fcb244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 28, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = torch.mean(poses_features, dim=[3])\n",
    "pooled.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d826424d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_poses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e285de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_poses = torch.Tensor([[[0,0,1],\n",
    "                            [0,1,1],\n",
    "                            [1,0,1],\n",
    "                            [1,1,1]]])\n",
    "target_poses = 0.5*init_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3a1bf346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2]) torch.Size([1, 4, 1])\n",
      "tensor([[[0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.8750, 1.0000],\n",
      "         [0.8750, 0.0000, 1.0000],\n",
      "         [0.8750, 0.8750, 1.0000]]])\n",
      "torch.Size([1, 4, 2]) torch.Size([1, 4, 1])\n",
      "tensor([[[0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.7500, 1.0000],\n",
      "         [0.7500, 0.0000, 1.0000],\n",
      "         [0.7500, 0.7500, 1.0000]]])\n",
      "torch.Size([1, 4, 2]) torch.Size([1, 4, 1])\n",
      "tensor([[[0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.6250, 1.0000],\n",
      "         [0.6250, 0.0000, 1.0000],\n",
      "         [0.6250, 0.6250, 1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "v = init_poses[:,:,2].unsqueeze(-1)\n",
    "num_iterations = 3\n",
    "for it in range(1, num_iterations+1):\n",
    "    w = it/(num_iterations+1)\n",
    "    tmp = torch.lerp(input=init_poses[:,:,0:2], end=target_poses[:,:,0:2], weight=w)\n",
    "    print(tmp.shape, v.shape)\n",
    "    tmp = torch.cat([tmp, v], dim=-1)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1102d04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3320"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbx = [45, 2, 85, 85]\n",
    "area = abs(bbx[2]-bbx[0])*abs(bbx[3]-bbx[1])\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8c685e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_poses = torch.cat([init_poses, init_poses], dim=0)\n",
    "target_poses = torch.cat([target_poses,target_poses], dim=0)\n",
    "areas = torch.randn(input_poses.shape[0], 1)\n",
    "weights = torch.randn(input_poses.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "538f9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([4]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "areas.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fd03bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([ 0.0000, -0.6091,  0.4357, -0.5313])\n",
      "torch.Size([4])\n",
      "tensor([ 0.0000, -0.3583,  0.2563, -0.3126])\n"
     ]
    }
   ],
   "source": [
    "input_x, input_y = input_poses[:,:,0], input_poses[:,:,1]\n",
    "target_x, target_y = target_poses[:,:,0], target_poses[:,:,1]\n",
    "visibilities = target_poses[:,:,2].to(torch.uint8)\n",
    "offsets = (input_x-target_x)**2 + (input_y-target_y)**2\n",
    "offsets /= areas\n",
    "for i in range(offsets.shape[0]):\n",
    "    print(offsets[i].shape)\n",
    "    offsets[i] = offsets[i]/(2*weights)\n",
    "    print(offsets[i])\n",
    "oks = torch.exp(exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c216895a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4055, 1.4055, 1.9754],\n",
       "        [1.0000, 1.0690, 1.0690, 1.1428],\n",
       "        [1.0000, 0.9796, 0.9796, 0.9597],\n",
       "        [1.0000, 0.9640, 0.9640, 0.9293]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7800e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ff22438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, **y: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "609eb8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, **y)>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "24465bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.4055, 1.4055, 1.9754],\n",
       "        [1.0000, 1.0690, 1.0690, 1.1428],\n",
       "        [1.0000, 0.9796, 0.9796, 0.9597],\n",
       "        [1.0000, 0.9640, 0.9640, 0.9293]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(oks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd821002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c5ed7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oks(nn.Module):\n",
    "    def __init__(self, weights: Tensor, normalize: bool):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        input_poses: Tensor, \n",
    "        target_poses: Tensor, \n",
    "        areas: Tensor=None\n",
    "    ):\n",
    "        dists = ((input_poses[:,:,0:2] - target_poses[:,:,0:2])**2).sum(dim=-1)\n",
    "        visible = target_poses[:,:,2].to(torch.bool)\n",
    "        if self.normalize:\n",
    "            assert areas is not None, f'`areas` is required when `normalize=True`'\n",
    "            if len(areas.shape) < 2: areas = areas.unsqueeze(-1)\n",
    "            dists /= areas\n",
    "        if len(self.weights.shape) < 2: self.weights = self.weights.unsqueeze(0)\n",
    "        dists /= 2*self.weights\n",
    "        dists[~visible] = 0.\n",
    "        oks = torch.exp(-dists).mean(1)\n",
    "        return oks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b1d687c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 14\n",
    "N = 100\n",
    "\n",
    "input_poses = torch.randn(N, K, 3)\n",
    "target_poses = torch.randn(N, K, 3)\n",
    "areas = torch.randn(N)\n",
    "weights = torch.full((K,), 1/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15472950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "fe65a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "oks_fn = Oks(weights, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f30c413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([       inf, 6.8006e+15, 1.2298e-01, 1.3482e-01, 2.8744e-02,        inf,\n",
       "        2.2375e-02,        inf, 9.8292e-02, 8.2052e-02, 2.6378e-02, 2.4942e+33,\n",
       "        2.5937e+31, 4.2028e-05, 2.2975e+28,        inf, 1.0559e-01,        inf,\n",
       "               inf,        inf,        inf,        inf, 6.4847e-04,        inf,\n",
       "        4.2245e-02, 5.8403e+16,        inf, 2.4163e-25, 2.5934e+21, 6.2382e-03,\n",
       "        4.0754e-05,        inf, 2.7037e-03, 2.2665e+27,        inf, 0.0000e+00,\n",
       "        9.5822e-04,        inf,        inf, 2.3654e-03, 2.0441e-27, 1.3375e-05,\n",
       "        1.2480e+21, 7.7099e-05, 1.1209e-02, 2.2767e-06, 1.0175e+22, 3.1317e-04,\n",
       "        3.1393e-02,        inf, 1.0848e-02, 1.6526e-02, 1.4833e-03, 3.3481e-06,\n",
       "        1.5458e-01, 7.2229e-04,        inf, 9.5561e+21,        inf, 9.5622e-03,\n",
       "        6.7893e+29,        inf, 1.5659e-02, 6.8765e-08, 6.8272e-03,        inf,\n",
       "        1.2151e+24, 1.2326e-08, 7.7308e-08,        inf, 1.7343e-04,        inf,\n",
       "        8.0377e-06, 1.4623e+22, 1.9225e-04,        inf, 2.4585e-05,        inf,\n",
       "        1.4403e+17, 1.8562e-03, 5.4582e-03, 1.2874e+31, 6.1399e+14, 6.0274e-03,\n",
       "        9.2052e-03, 1.8172e-05,        inf, 8.2967e-06, 7.7801e-02, 3.0955e-05,\n",
       "        2.8182e-02, 2.2473e-02, 1.2538e-06, 1.0245e-01, 6.4262e-04,        inf,\n",
       "               inf, 2.4232e-02, 1.5463e-01, 1.5614e-05])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oks_fn(input_poses, target_poses, areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb250a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "4d08462b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bf0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1b46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe3bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dffd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bae42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8cb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd145ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff9d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
